{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FaadnhbpCcsh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ly7lrx-gCuLy"
   },
   "source": [
    "## Dades\n",
    "\n",
    "Emprarem el dataset **EMNIST**. És un conjunt de dígits de caràcters escrits a mà derivats de la base de dades del NIST 19 i convertits a un format d'imatge seguint una estructura que coincideix directament amb el conjunt de dades del MNIST. Aquest conjunt de dades presenta múltiples subconjunts nosaltres en primer lloc emprarem el dels _digits_. [Documentacio](https://pytorch.org/vision/main/generated/torchvision.datasets.EMNIST.html)\n",
    "\n",
    "\n",
    "\n",
    "Cada un dels elements del dataset és una imatge de 28x28 pixels i pot ser de 47 classes diferents. La descripció és al següent article: [enllaç](https://arxiv.org/pdf/1702.05373v1.pdf)\n",
    "La càrrega i preparació de les dades segueix la mateixa estructura que quan fèiem aprenentatge emprant SVM, afegint una passa més, la creació de subconjunts d'entrenament (també coneguts com _mini-batches_).\n",
    "\n",
    "1. Càrrega de dades.\n",
    "2. Estandarització.\n",
    "3. Creació de grups d'entrenament.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwSoPhjXCvV9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# El label del dataset és l'índex de la llista labels. Cada posició de la llista és un codi ASCII. Podeu emprar la funció chr per fer la transformació\n",
    "\n",
    "# Definim una seqüència (composició) de transformacions\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # mitjana, desviacio tipica (precalculats)\n",
    "    ])\n",
    "\n",
    "# Descarregam un dataset ja integrat en la llibreria Pytorch:\n",
    "train = datasets.EMNIST('data', split=\"digits\", train=True, download=True, transform=transform)  ## Si acabau podeu fer proves amb el split \"balanced\"\n",
    "test = datasets.EMNIST('data', split=\"digits\",train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com és un dataset? Quina estructura té?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Info:\")\n",
    "print(\"Tipus de la variable train : \", type(train)) # la variable test te les mateixes característiques\n",
    "print(test.__dict__.keys()) ## Tot objecte Python té un diccionari amb els seus atributs\n",
    "classes = test.classes ## Obtenim una llista amb les classes del dataset\n",
    "print(\"Classes:\"+\"=\"*50)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-vdST97JpfB"
   },
   "source": [
    "És molt important entendre com és l'estructura dels conjunts que necessitam per fer feina amb la xarxa.\n",
    "La classe `DataLoader` rep un conjunt de dades i una mida de `batch`, i ens proporciona un iterable sobre aquest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHy3Yd6C87Fz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_batch_size = 64\n",
    "test_batch_size = 100\n",
    "\n",
    "# Transformam les dades en l'estructura necessaria per entrenar una xarxa\n",
    "train_loader = torch.utils.data.DataLoader(train, train_batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test, test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmCCiKz29MOe"
   },
   "source": [
    "\n",
    "A continuació cream un iterador sobre el nostre conjunt d'entrenament, això ens torna un _batch_.\n",
    "Mostrarem la primera imatge juntament amb la seva etiqueta. Després mostram informació referent al _batch_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yispMZfr1bJn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "iterador =  iter(train_loader) # Un iterador!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "He-cj6JuFsgr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "features, labels = next(iterador)\n",
    "\n",
    "# TODO: mostrar una imatge del batch i com a títol posar l'etiqueta.\n",
    "# Extra: mostrar una graella amb tot el batch·\n",
    "\n",
    "print(\"Saber l'estructura del batch us ajudarà: \")\n",
    "print(f\"Feature batch shape: {features.size()}\")\n",
    "print(f\"Labels batch shape: {labels.size()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8i4Mg8KuD3r"
   },
   "source": [
    "## Definició de la xarxa\n",
    "Cma ja sabem (almanco ens sona de la classe anterior) emprant el mòdul `nn` de _Pytorch_ podem definir la nostra pròpia xarxa, en aquesta pràctica realitzarem una classe que la contengui, això ens dona més llibertat i flexibilitat.\n",
    "\n",
    "Crearem una classe filla de `nn.Module` on com a mínim hi definirem dos mètodes:\n",
    "\n",
    "  - Constructor: mètode `__init__` en el que definim les capes de la nostra xarxa.\n",
    "  - `forward`: mètode en el qual definim com és el flux de la informació. Aquí podem afegir capes no entrenables, per exemple una `ReLu`.\n",
    "\n",
    "La xarxa que heu de crear és una xarxa densa, per tant, totes les capes seran linears: `nn.Linear`. On la darrera capa ha de tenir una dimensionalitat igual al nombre de classes que volem predir.\n",
    "\n",
    "Com a sortida és recomanable usar la funció _softmax_ que converteix un vector de $K$ nombres reals en una distribució de probabilitat de $K$ possibles resultats. És una generalització de la funció logística a múltiples dimensions, i s'utilitza en regressió logística multinomial. La funció _softmax_ s'utilitza sovint com l'última funció d'activació d'una xarxa neural per normalitzar la seva sortida a una distribució de probabilitat sobre les classes de sortida predites. Es calcula de la següent manera:\n",
    "\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/ab3ef6ba51afd36c1d2baf06540022053b2dca73\"\n",
    "     alt=\"Softmax\"\n",
    "     style=\"float: left; margin-right: 10px;\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bz9rSkjSuF6r",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(784, #¿?) # 28x28 = 784\n",
    "\n",
    "        # TODO: definir les capes que necessitem\n",
    "\n",
    "    def forward(self, x):\n",
    "            \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.l1(x) #\n",
    "\n",
    "        # TODO connectar les capes. El valor de retorn d'una capa és l'entrada de la següent\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6ISOL_hCk7g"
   },
   "source": [
    "## Entrenament\n",
    "\n",
    "Les bones pràctiques de programació ens diuen que cal organitzar el codi en funcions. En definirem una per la passa d'entrenament i una altra per la fase de test.\n",
    "\n",
    "En aquesta fase s'ha de definir la funció de pèrdua, recordau que aquesta és la funció que avalua la diferència entre el que ha predit la xarxa i la sortida desitjada. Existeixen múltiples funcions de pèrdua que emprarem segons el problema a resoldre i les seves particularitats. Per exemple en el problema de regressió de la setmana passada vàrem emprar l'error absolut al quadrat (_MSE_).\n",
    "\n",
    "Nosaltres emprarem: **cross entropy** que prové de la teoria de la informació de _Shannon_ i que vé a explicar que l'entropia d'una variable aleatòria és el nivell mitjà d'informació / sorpresa / incertesa inherent als possibles resultats d'aquesta variable. La fórmula és la següent:\n",
    "\n",
    "$ LCE = -∑_{i=1}^M t_i \\log(p_i)$\n",
    "\n",
    "On $M$ és el nombre de classes, $t_i$ és un valor binari indicant si l'observació és d'aquesta classe (valor 1 si ho és i valor 0 en cas contrari) i $p_i$ és el resultat de la funció _Softmax_ per aquesta classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9OLtpPzClch",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, loss_fn, log_interval=100, verbose=True):\n",
    "    \n",
    "    model.train() # Posam la xarxa en mode entrenament\n",
    "\n",
    "    loss_v = 0 # Per calcular la mitjana (és la vostra)\n",
    "\n",
    "    # Bucle per entrenar cada un dels batches\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    \n",
    "        data, target = data.to(device), target.to(device)  ###  Veure ús de CuDA en cel·les inferiors\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss =  loss_fn(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        ## Informació de debug\n",
    "        if batch_idx % log_interval == 0 and verbose:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Average: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(), loss.item()/ len(data)))\n",
    "        loss_v += loss.item()\n",
    "\n",
    "    loss_v /= len(train_loader.dataset)\n",
    "    print(f'\\n Epoch {epoch} Train set: Average loss: {loss_v}')\n",
    " \n",
    "    return loss_v\n",
    "\n",
    "\n",
    "def test(model, device, test_loader,loss_fn):\n",
    "    model.eval() # Posam la xarxa en mode avaluació\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad(): # desactiva el càlcul de gradients, no ho necessitam per l'inferència. Estalvia memòria i fa més ràpids els càlculs\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss +=  loss_fn(output, target)\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # index amb la max probabilitat\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    " \n",
    "    # Informació de debug\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBGKL43vsUnD"
   },
   "source": [
    "A continuació definim els paràmetres d'entrenament i el bucle principal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNIBWqAwsVSb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(33)\n",
    "\n",
    "# El següent ens permet emprar l'entorn de cuda. Si estam emprant google colab el podem activar a \"Entorno de ejecución\"\n",
    "use_cuda = True\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Paràmetres bàsics\n",
    "epochs = #  ¿?\n",
    "lr =  #  ¿?\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "# Stochastic gradient descent\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Guardam el valor de pèrdua mig   de cada època, per fer el gràfic final\n",
    "train_l = np.zeros((epochs))\n",
    "test_l = np.zeros((epochs))\n",
    "\n",
    "loss_fn =  #¿?\n",
    "\n",
    "# Bucle d'entrenament\n",
    "for epoch in range(0, epochs):\n",
    "    train_l[epoch] = train(model, device, train_loader, optimizer, epoch, loss_fn, verbose=False)\n",
    "    test_l[epoch]  = test(model, device, test_loader, loss_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostram la gràfica resultat de l'entrenament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFy6vECD8Pbq"
   },
   "outputs": [],
   "source": [
    "plt.title(\"Resultats de l'entrenament\")\n",
    "plt.plot(range(1, (epochs + 1)), train_l,  c=\"red\", label=\"train\")\n",
    "plt.plot(range(1,  (epochs + 1)), test_l,  c=\"green\", label=\"test\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaluam amb una mètrica objectiva (_accuracy_)\n",
    "\n",
    "Per fer això hem de recorrer tots els _batch_ del conjunt de test i avaluar-los. Finalment empram la mètrica ja coneguda de la llibreria _sklearn_ per calcular-la:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model.eval() # Posam la xarxa en mode avaluació\n",
    "\n",
    "targets = []\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad(): # desactiva el càlcul de gradients, no ho necessitam per l'inferència. Estalvia memòria i fa més ràpids els càlculs\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # index amb la max probabilitat\n",
    "        targets.extend(target.tolist())\n",
    "        predictions.extend(torch.flatten(pred.cpu()).tolist())\n",
    "        \n",
    "targets = np.asarray(targets)\n",
    "predictions = np.asarray(predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(targets, predictions)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
