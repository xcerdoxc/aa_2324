{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pràctica I - Perceptron\n",
    "\n",
    "F. Rosenblatt, 1958 - Basat en el model neuronal proposat per McCulloch-Pitts.\n",
    "\n",
    "És un algorisme que **aprèn de manera òptima els coeficients dels pesos** de la neurona que un cop multiplicats per\n",
    "les característiques d'entrada permet determinar si aquesta s'ha d'activar o no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea bàsica\n",
    "\n",
    "Aquest model permet resoldre una tasca de classificació binària, és a dir on tenim 2 classes, representades amb les etiquetes 1 i -1, en la que definim una funció d'activació $\\phi(z)$ que es calcula com una combinació lineal d'un vector de característiques $\\mathbf{x}$ y un vector de pesos $\\mathbf{w}$. On $z = w_1x_1 + \\ldots + w_mx_m$.\n",
    "\n",
    "Llavors, donat un exemple $x^{(i)}$, si la sortida de $\\phi(z)$ és major que un determinat valor llindar (en anglés _threshold_ ) $\\theta$ podem predir que aquest pertany a la classe 1 i en cas contrari direm que pertany a la classe -1. Això es coneix com a funció escaló (en anglès *step function*).\n",
    "\n",
    "$\\phi(z) = \\begin{cases}\n",
    " 1 & \\text{if z} >= \\theta, \\\\\n",
    "-1 & \\text{ en cas contrari}\n",
    "\\end{cases}$\n",
    "\n",
    "A efectes pràctics, podem moure el _threshold_ a la part esquerra de l'equació i definir un pes $w_0 = -\\theta$ i $x_0 = 1$  d'aquesta manera podem escriure $\\mathbf{z}$ com a:  $z = w_0x_0 + w_1x_1 + \\ldots + w_mx_m = w^Tx$ i llavors:\n",
    "\n",
    "\n",
    "$\\phi(z) = \\begin{cases}\n",
    " 1 & \\text{if z} >= 0, \\\\\n",
    "-1 & \\text{ en cas contrari}\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorisme\n",
    "\n",
    "L'algorisme es resumeix en les següents pases:\n",
    "\n",
    "1. Inicialitzar els pesos a valor 0 (o a un nombre aleatori prou petit).\n",
    "\n",
    "2. Fer $n$ iteracions. En cada iteració ajustarem els pesos del perceptron:\n",
    "\n",
    "    2.1 Per cada element, $x^{(i)}$, del conjunt que emprarem per entrenar fer:\n",
    "        1. Calcular la classe de sortida (predicció) de la xarxa $\\hat y$.\n",
    "        2. Actualitzar el vector de pesos, $\\mathbf{w}$.\n",
    "\n",
    "3. Calcular la classe de sortida (predicció) *final* de la xarxa $\\hat y$.\n",
    "\n",
    "### Actualitzar el vector de pesos\n",
    "\n",
    "L'actualització d'un element $w_j$ del vector de pesos $\\mathbf{w}$ es pot expressar de la següent manera:\n",
    "\n",
    "$$w_j = w_j + \\Delta w_j$$\n",
    "\n",
    "El valor de $\\Delta w_j$ es calcula mitjançant la norma d'aprenentatge del perceptron:\n",
    "\n",
    "$$\\Delta w_j = \\eta(y^{(i)} - \\hat y^{(i)}) x_j^{(i)}$$\n",
    "\n",
    "on $\\eta$ és la ràtio d'aprenentatge (una constant entre 0.0 i 1.0), $y^{(i)}$ és la classe per l'exemple ièssim i  $\\hat y^{(i)}$ és la classe que el classificador ha predit.\n",
    "\n",
    "\n",
    "*Nota*: Aquest algorisme només convergeix si les dues classes són separables linealment i la ràtio d'aprenentatge és prou petit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Feina a fer\n",
    "\n",
    " 0. Entendre l'algorisme del perceptron.\n",
    " 1. Implementar els mètodes _fit_, i _predict_ de la classe Perceptron.\n",
    " 2. Entrenar un perceptron amb el conjunt de dades que trobareu en el fitxer _main_.\n",
    " 3. Validar l'entrenament mostrant la recta de divisió del conjunt.  [Veure documentació axline](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.axline.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "\n",
     "## Algorisme\n",
     "\n",
     "L'algorisme es resumeix en les següents pases:\n",
     "\n",
     "1. Inicialitzar els pesos a valor 0 (o a un nombre aleatori prou petit).\n",
     "\n",
     "2. Fer $n$ iteracions on ajustarem els pesos:\n",
     "\n",
     "    - Per cada element, $x^{(i)}$, del conjunt que emprarem per entrenar fer:\n",
     "        1. Calcular la classe de sortida (predicció) de la xarxa $\\hat y$.\n",
     "\n",
     "        2. Actualitzar el vector de pesos, $\\mathbf{w}$.\n",
     "\n",
     "3. Calcular la classe de sortida (predicció) *final* de la xarxa $\\hat y$.\n",
     "\n",
     "### Actualitzar el vector de pesos\n",
     "\n",
     "L'actualització d'un element $w_j$ del vector de pesos $w$ es pot expressar de la següent manera:\n",
     "\n",
     "$$w_j = w_j + \\Delta w_j$$\n",
     "\n",
     "El valor de $\\Delta w_j$ es calcula mitjançant la norma d'aprenentatge del perceptron:\n",
     "\n",
     "$$\\Delta w_j = \\eta(y^{(i)} - \\hat y^{(i)}) x_j^{(i)}$$\n",
     "\n",
     "on $\\eta$ és el rati d'aprenentatge (una constant entre 0.0 i 1.0), $y^{(i)}$ és la classe per l'exemple ièssim i\n",
     " $\\hat y^{(i)}$ és la classe que el classificador ha predit.\n",
     "\n",
     "\n",
     "*Nota*: Aquest algorisme només convergeix si les dues classes són separables linealment i el rati d'aprenentatge és\n",
     "suficientment petit.\n"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
